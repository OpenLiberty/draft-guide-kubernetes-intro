// INSTRUCTION: Please remove all comments that start INSTRUCTION prior to commit. Most comments should be removed, although not the copyright.
// INSTRUCTION: The copyright statement must appear at the top of the file
//
// Copyright (c) 2018 IBM Corporation and others.
// Licensed under Creative Commons Attribution-NoDerivatives
// 4.0 International (CC BY-ND 4.0)
//   https://creativecommons.org/licenses/by-nd/4.0/
//
// Contributors:
//     IBM Corporation
//
:projectid: kubernetes
:page-layout: guide
:page-duration: 25 minutes
:page-releasedate: 2018-05-30
:page-description: Explore how to deploy microservices to Kubernetes and manage your cluster.
:page-tags: ['microservices', 'Kubernetes', 'Docker', 'containers', 'kubectl', 'Minikube']
:page-permalink: /guides/{projectid}
:page-related-guides: ['docker', 'istio']
:common-includes: https://raw.githubusercontent.com/OpenLiberty/guides-common/master
:source-highlighter: prettify
= Deploying microservices to Kubernetes

[.hidden]
NOTE: This repository contains the guide documentation source. To view the guide in published form, view it on the https://openliberty.io/guides/{projectid}.html[Open Liberty website].

Explore how to deploy microservices in Open Liberty Docker containers to Kubernetes and manage them with the kubectl Kubernetes CLI.

:minikube-ip: 192.168.99.100
:kube: Kubernetes
:hashtag: #
:win: WINDOWS
:mac: MAC
:linux: LINUX
:name-api: http://[hostname]:31000/api/name
:ping-api: http://[hostname]:32000/api/ping


// =================================================================================================
// What is {kube}
// =================================================================================================

== What is {kube}?

{kube} is an open source container orchestrator that automates many tasks involved in deploying,
managing, and scaling containerized applications.

Over the years, {kube} has become a major tool in containerized environments as containers are being
more and more utilized for all steps of a continuous delivery pipeline.

=== Why use {kube}?

Managing containers on their own can be challenging. While a few containers used for development by
a small team might not pose a problem, a large number of containers, all running various applications,
all sharing databases, all communicating with each other, and all needing to be run and monitored
every day of the week will give even a large team of experienced developers a headache. {kube} is a
a primary tool for development in containerized
environments. It handles scheduling, deployment, as well as mass deletion and creation of containers.
It provides update rollout capabilities on a large scale that would otherwise prove extremely tedious
to do. Imagine that you updated a Docker image, which now needs to propagate to a dozen containers.
While you could destroy and then recreate these containers, you could also issue a short one-line
command and have {kube} do all that updating for you. Of course this is just a simple example, the
iceberg that is {kube} has a lot more to offer.

// {kube} provides much other functionality
// that simply makes your development, testing, and production environments much more stable, much more
// automated, and much more powerful to use. Some of this functionality, we're hoping you can learn in
// this guide.

=== Architecture

Deploying an application to Kubernetes simply means deploying an application to a Kubernetes cluster.

A typical {kube} cluster is a collection of physical or virtual machines called `nodes` that run
containerized applications. A cluster is made up of one master `node` that manages the cluster, and
multiple worker `nodes` that run the actual application instances inside {kube} objects called `Pods`.

A `Pod` is a basic building block in a {kube} cluster. It represents a single running process that
encapsulates a container or in some scenarios multiple closely coupled containers. `Pods` can be
replicated to scale applications and handle more traffic. From the perspective of a cluster, a set
of replicated `Pods` is still one application instance, although it might be made up of dozens of
instances of itself. A single `Pod` or a group of replicated `Pods` are managed by {kube} objects
called `Controllers`. A `Controller` handles replication, self-healing, rollout of updates, and general
management of `Pods`. Some examples of `Controllers` include `Deployments`, `StatefulSets`, and `DaemonSets`.
In this guide, you will work with `Deployments`.

A `Pod` or a group of replicated `Pods` are abstracted through {kube} objects called `Services`
that define a set of rules by which the `Pods` can be accessed. In a basic scenario, a {kube}
`Service` exposes a node port that can be used together with the cluster IP address to access
the `Pods` encapsulated by the `Service`.

To learn about the various Kubernetes resources that you can configure, see the https://kubernetes.io/docs/concepts/.


// =================================================================================================
// Introduction
// =================================================================================================

== What you'll learn

You will learn how to deploy two microservices in Open Liberty Docker containers to a local {kube} cluster.
You will then manage your deployed microservices using the `kubectl` command line
interface for {kube}. The `kubectl` CLI is your primary tool for communicating with and managing your
{kube} cluster.

The two microservices you will deploy are called `name` and `ping`. The `name` microservice simply
displays a brief greeting and the name of the
container that it runs in, making it easy to distinguish it from its other replicas. The `ping` microservice
simply pings the {kube} Service that encapsulates the `Pods` running the `name` microservice, demonstrating
how communication can be established between `Pods` inside a cluster.

You will use a local single-node {kube} cluster.


// =================================================================================================
// Prerequisites
// =================================================================================================

== Prerequisites

Before you begin, make sure to have the following tools installed:

First, you will need a containerization software for building containers. {kube} supports a variety
of container types. You will use `Docker` in this guide. For installation instructions, see https://docs.docker.com/install/.

****
[system]#*{win} | {mac}*#

You will use the Docker Desktop, where a local {kube} environment is pre-installed and enabled. Make sure you have the `Docker` version with the Docker Desktop support.

- Setup Docker for Windows as described at https://docs.docker.com/docker-for-windows/#kubernetes.
- Setup Docker for Mac as described at https://docs.docker.com/docker-for-mac/#kubernetes.
- After following one of the sets of instructions, ensure that {kube} (not Swarm) is selected as the orchestrator.

[system]#*{linux}*#

You will use `Minikube` as a single-node {kube} cluster that runs locally in a virtual machine.
For Minikube installation instructions see https://github.com/kubernetes/minikube. Make sure to read the "Requirements" section as different operating systems require different prerequisites to get Minikube running.
****

// =================================================================================================
// Getting Started
// =================================================================================================

include::{common-includes}/gitclone.adoc[]

// no "try what you'll build" section in this guide since it would be too long due to all setup the user will have to do.


// =================================================================================================
// Staring and preparing your cluster for deployment
// =================================================================================================

== Starting and preparing your cluster for deployment

Start up your {kube} cluster.

****
[system]#*{win} | {mac}*#

Start your Docker Desktop environment.

[system]#*{linux}*#

Run the following command from a command line:

```
minikube start
```

Later, when you no longer need your cluster, you can stop it with `minikube stop` and delete it completely
with `minikube delete`.
****

Next, validate that you have a healthy kubernetes environment by running the following command from the command line:
```
kubectl get nodes
```

This should return a `Ready` status for the master node.

****
[system]#*{win} | {mac}*#

You do not need to do any other step.

[system]#*{linux}*#

Run the following command to configure the Docker CLI to use Minikube's Docker daemon.
By running this command, you will be able to interact with Minikube's Docker daemon and build new
images directly to it from your host machine:

```
eval $(minikube docker-env)
```

When you no longer want to use Minikube's Docker daemon, run the following command to point back to your host:

```
eval $(minikube docker-env -u)
```
****


// =================================================================================================
// Building and containerizing the microservices
// =================================================================================================

== Building and containerizing the microservices

The first step of deploying to {kube} is to build your microservices and containerize them with Docker.

The starting Java project, which you can find in the `start` directory, is a multi-module Maven
project that's made up of the `name` and `ping` microservices, each residing in their own directories,
`start/name` and `start/ping`. Each of these directories also contains a Dockerfile, which is necessary
for building Docker images. If you're unfamiliar with Dockerfiles, check out the
https://openliberty.io/guides/docker.html[Using Docker containers to develop microservices] guide,
which covers Dockerfiles in depth.

If you're familiar with Maven and Docker, you might be tempted to run a Maven build first and then
use the `.war` file produced by the build to build a Docker image. While this is by no means a wrong
approach, we've setup the projects such that you can build your microservices and Docker image simultaneously
as a part of a single Maven build. This is done using the `dockerfile-maven` plugin, which automatically
picks up the Dockerfile located in the same directory as its POM file and builds a Docker image from it.
If you are using Docker for Windows ensure that, on the Docker for Windows _General Setting_ page, the option is set to `Expose daemon on tcp://localhost:2375 without TLS`. This is required by the `dockerfile-maven` part of the build.

Navigate to the `start` directory and run the following command:

```
mvn package
```

The `package` goal automatically invokes the `dockerfile-maven:build` goal, which runs during the
`package` phase. This goal builds a Docker image from the Dockerfile located in the same directory
as the POM file.

During the build, you'll see various Docker messages describing what images are being downloaded and
built. When the build finishes, run the following command to list all local Docker images:

```
docker images
```

Verify that the `name:1.0-SNAPSHOT` and `ping:1.0-SNAPSHOT` images are listed among them, for example:

****
[system]#*{win} | {mac}*#

[source, role="no_copy"]
----
REPOSITORY                                                       TAG
ping                                                             1.0-SNAPSHOT
name                                                             1.0-SNAPSHOT
open-liberty                                                     latest
k8s.gcr.io/kube-proxy-amd64                                      v1.10.3
k8s.gcr.io/kube-scheduler-amd64                                  v1.10.3
k8s.gcr.io/kube-controller-manager-amd64                         v1.10.3
k8s.gcr.io/kube-apiserver-amd64                                  v1.10.3
k8s.gcr.io/etcd-amd64                                            3.1.12
k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64                           1.14.8
k8s.gcr.io/k8s-dns-sidecar-amd64                                 1.14.8
k8s.gcr.io/k8s-dns-kube-dns-amd64                                1.14.8
k8s.gcr.io/pause-amd64                                           3.1
gcr.io/google_containers/defaultbackend                          1.4
----

[system]#*{linux}*#

[source, role="no_copy"]
----
REPOSITORY                                                       TAG
ping                                                             1.0-SNAPSHOT
name                                                             1.0-SNAPSHOT
open-liberty                                                     latest
k8s.gcr.io/kube-proxy-amd64                                      v1.10.0
k8s.gcr.io/kube-controller-manager-amd64                         v1.10.0
k8s.gcr.io/kube-apiserver-amd64                                  v1.10.0
k8s.gcr.io/kube-scheduler-amd64                                  v1.10.0
quay.io/kubernetes-ingress-controller/nginx-ingress-controller   0.12.0
k8s.gcr.io/etcd-amd64                                            3.1.12
k8s.gcr.io/kube-addon-manager                                    v8.6
k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64                           1.14.8
k8s.gcr.io/k8s-dns-sidecar-amd64                                 1.14.8
k8s.gcr.io/k8s-dns-kube-dns-amd64                                1.14.8
k8s.gcr.io/pause-amd64                                           3.1
k8s.gcr.io/kubernetes-dashboard-amd64                            v1.8.1
k8s.gcr.io/kube-addon-manager                                    v6.5
gcr.io/k8s-minikube/storage-provisioner                          v1.8.0
gcr.io/k8s-minikube/storage-provisioner                          v1.8.1
k8s.gcr.io/defaultbackend                                        1.4
k8s.gcr.io/k8s-dns-sidecar-amd64                                 1.14.4
k8s.gcr.io/k8s-dns-kube-dns-amd64                                1.14.4
k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64                           1.14.4
k8s.gcr.io/etcd-amd64                                            3.0.17
k8s.gcr.io/pause-amd64                                           3.0
----
****

If you don't see the `name:1.0-SNAPSHOT` and `ping:1.0-SNAPSHOT` images, then check the Maven
build log for any potential errors.
In addition, if you are using Minikube, make sure that your Docker CLI is configured to use Minikube's Docker daemon and not your host's as described in the previous section.


// =================================================================================================
// Deploying the applications
// =================================================================================================

== Deploying the applications

Now that your Docker images are built, deploy them using a kubernetes resource definition.

A kubernetes resource definition is a yaml file that contains a description of all your deployments, services, or any other resources that you wish to deploy. All resources can also be deleted from the cluster by using the same yaml file that you used to deploy them.

To deploy the `name` and `ping` applications, first create the `kubernetes.yaml` file in the `start` directory:

[source, yaml]
----
include::finish/kubernetes.yaml[tags=**;]
----

This file defines four {kube} resources. It defines two deployments and two services. A {kube} Deployment is a resource responsible for controlling the creation and management of `Pods`. A service exposes your deployment so that you can make requests to your containers. Three key items to look at when creating the deployments are the `label`, `image`, and `containerPort` fields. The `label` is a way for a {kube} service to reference specific deployments. The `image` is simply the name and tag of the docker image that you want to use for this container. Finally, the `containerPort` is the port that your container exposes for purposes of accessing your application. For the services the key point to understand is that they expose your deployments, which is specified by the use of labels -- in this case the `app` label. You will also notice that the service has a type of `NodePort`. This means that you will be able to access these services from outside of your cluster via a specific port. In this case, the ports will be `31000` and `32000`, but this can also be randomized if the `nodePort` field is not used.

Run the following commands to deploy the resources as defined in kubernetes.yaml:

```
kubectl apply -f kubernetes.yaml
```

When the apps are deployed, run the following command to check the status of your `Pods`:

```
kubectl get pods
```

You'll see an output similar to the following if all the `Pods` are healthy and running:

[source, role="no_copy"]
----
NAME                               READY     STATUS    RESTARTS   AGE
name-deployment-6bd97d9bf6-4ccds   1/1       Running   0          15s
ping-deployment-645767664f-nbtd9   1/1       Running   0          15s
----

You can also inspect individual `Pods` in more detail by running the following command:

```
kubectl describe pods
```

You can also issue the `kubectl get` and `kubectl describe` commands on other {kube} resources, so feel
free to inspect all other resources.

Next you will make requests to your services.

****
[system]#*{win} | {mac}*#

The default hostname for Docker Desktop is `localhost`.

[system]#*{linux}*#

The default hostname for minikube is {minikube-ip}. Otherwise it can be found using the `minikube ip` command.
****

Then `curl` or visit the following URLs to access your microservices, substituting the appropriate hostname:

- {name-api}
- {ping-api}/name-service

The first URL returns a brief greeting followed by the name of the `Pod` that the `name` microservice
runs in. The second URL returns `pong` if it received a good response from the `name-service`
{kube} Service. Visiting `{ping-api}/[kube-service]` in general returns either
a good or a bad response depending on whether `kube-service` is a valid {kube} Service that can be accessed.

// =================================================================================================
// Scaling a deployment
// ================================================================================================

== Scaling a deployment

To make use of load balancing, you need to scale your deployments. When you scale a Deployment, you replicate its `Pods`, creating more running instances of your applications. Scaling is one of the primary advantages of {kube} because replicating your application allows it to accommodate more traffic, and then descale your Deployments to free up resources when the traffic decreases.

As an example, scale the `name` Deployment to 3 `Pods` by running the following command:

```
kubectl scale deployment/name-deployment --replicas=3
```

Wait for your two new `Pods` to be in the ready state, then `curl` or visit the `{name-api}` URL. You'll notice that the service will respond with a different name when you call it multiple times. This is because there are now three `Pods` running all serving the `name` application.

== Redeploy microservices

When you are building your application, you may find that you want to quickly test a change. To do that, you can rebuild your docker images then delete and re-create your {kube} resources.

```
mvn package
kubectl delete -f kubernetes.yaml
kubectl apply -f kubernetes.yaml
```

This is not how you would want to update your applications when running in production, but in a development environment this is fine. If you want to deploy an updated image to a production cluster, you can update the container in your deployment with a new image and {kube} will automate the creation of a new container and decomissioning of the old one once the new container is ready.

// =================================================================================================
// Testing microservices that are running on {kube}
// =================================================================================================

== Testing microservices that are running on {kube}

A few tests are included for you to test the basic functionality of the microservices. If a test failure
occurs, then you might have introduced a bug into the code. To run the tests, wait for all `Pods` to be
in the ready state before proceeding further. The default properties defined in the `pom.xml` are:

[cols="15, 100", options="header"]
|===
| *Property*        | *Description*
| cluster.ip        | IP or hostname for your cluster, `{minikube-ip}` by default, which is appropriate when using Minikube.
| name.kube.service | Name of the {kube} Service wrapping the `name` `Pods`, `name-service` by default.
| name.node.port    | The NodePort of the {kube} Service `name-service`, 31000 by default.
| ping.node.port    | The NodePort of the {kube} Service `ping-service`, 32000 by default.
|===

Navigate back to the `start` directory.

****
[system]#*{win} | {mac}*#

Run the integration tests against a cluster running with a hostname of `localhost`:

```
mvn verify -Ddockerfile.skip=true -Dcluster.ip=localhost
```

[system]#*{linux}*#

Run the integration tests against a cluster running at the default Minikube IP address:

```
mvn verify -Ddockerfile.skip=true
```

You can also run the integration tests with an IP address of `192.168.99.101`:

```
mvn verify -Ddockerfile.skip=true -Dcluster.ip=192.168.99.101
```
****

The `dockerfile.skip` parameter is set to `true` in order to skip building a new Docker image.

If the tests pass, you'll see an output similar to the following for each service respectively:

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.name.NameEndpointTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.673 sec - in it.io.openliberty.guides.name.NameEndpointTest

Results :

Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
----

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.ping.PingEndpointTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.222 sec - in it.io.openliberty.guides.ping.PingEndpointTest

Results :

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0
----


// =================================================================================================
// Tear Down
// =================================================================================================

== Tearing down the environment

When you no longer need your deployed microservices, you can delete all {kube} resources by running the `kubectl delete` command:

```
kubectl delete -f kubernetes.yaml
```

****
[system]#*{win} | {mac}*#

Nothing more needs to be done for Docker Desktop.

[system]#*{linux}*#

Perform the following two steps to return your environment to a clean state.
Firstly, stop your Minikube cluster:

```
minikube stop
```

Then delete your cluster:

```
minikube delete
```

Finally, point the Docker daemon back to your local machine:

```
eval $(minikube docker-env -u)
```

****


// =================================================================================================
// finish
// =================================================================================================

== Great work! You're done!

You have just deployed two microservices to {kube}. You then scaled a microservice and ran integration tests against miroservices that are running in a {kube}
cluster.

// Include the below from the guides-common repo to tell users how they can contribute to the guide
include::{common-includes}/finish.adoc[]

// DO NO CREATE ANYMORE SECTIONS AT THIS POINT
// Related guides will be added in automatically here if you included them in ":page-related-guides"
